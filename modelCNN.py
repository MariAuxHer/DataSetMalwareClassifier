#!/opt/homebrew/bin/python3
import torch
import torch.nn as nn

# input_features ???? ... oh yes I nee dit for the linear anyways...
# add source vocab size
model = TwoLayerPerceptron(source_vocab_size, 64).to(device)


class TwoLayerPerceptron(nn.Module):
    def __init__(self, input_features: int, hidden_units: int, source_vocab_size: int, embedding_size: int = 64):
        super().__init__()

        self.source_embedding = nn.Embedding(source_vocab_size, embedding_size)
        self.conv_network = nn.Conv1D(
            in_channels,
            out_channels,
            kernel_size,
            stride=1,
            padding="same",
            dilation=1,
            groups=1,
            bias=True,
            padding_mode="zeros",
            device=None,
            dtype=None,
        )
        self.linear1 = nn.Linear(input_features, hidden_units)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(hidden_units, 9)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        anything = self.linear1(x)
        anything = self.relu(anything)
        anything = self.linear2(anything)
        return anything

import torch
import torch.nn as nn


def _test():
    BATCH_SIZE = 16
    SEQUENCE_LENGTH = 128
    VOCAB_SIZE = 100
    EMBEDDING_SIZE = 32
    NUM_FILTERS = 64
    NUM_CLASSES = 2
    kernel_size = (8, EMBEDDING_SIZE)

    model = CNNSequenceClassifier(VOCAB_SIZE, EMBEDDING_SIZE, NUM_FILTERS, kernel_size, NUM_CLASSES, NUM_FILTERS)
    x = torch.randint(VOCAB_SIZE, (SEQUENCE_LENGTH, BATCH_SIZE))
    y = model(x)
    print(y.shape)


class PositionalEncoding(nn.Module):
    def __init__(self, embed_dim: int, dropout: float, max_length: int = 1 << 15):
        """
        :param embed_dim: Number of elements in embedding vectors.
        :param dropout: Probability of zeroing an output element.
        :param max_length: Longest sequence length supported.
        """
        super().__init__()
        ln_10000 = 9.21034049987793
        positional_encoding = torch.zeros((max_length, embed_dim))
        positions = torch.arange(0, max_length).reshape(max_length, 1)
        scale = torch.exp(-torch.arange(0, embed_dim, 2) * ln_10000 / embed_dim)
        positional_encoding[:, 0::2] = torch.sin(positions * scale)
        positional_encoding[:, 1::2] = torch.cos(positions * scale)
        positional_encoding = positional_encoding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer("positional_encoding", positional_encoding)

    def forward(self, token_embedding: torch.Tensor):
        """
        :param token_embedding: Token embeddings shaped (sequence length, batch size, embedding size)
        :returns: Embeddings with positional encoding added, possibly with some dropouts.
        """
        return self.dropout(token_embedding + self.positional_encoding[: token_embedding.size(0), :])


class TransformerSequenceClassifier(nn.Module):
    def __init__(
        self,
        source_vocab_size: int,
        max_sequence_length: int,
        num_encoder_layers: int = 6,
        num_heads: int = 8,
        embedding_size: int = 64,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        activation: callable = nn.functional.relu,
        layer_norm_eps: float = 1e-5,
    ):
        super().__init__()

        self.source_embedding = nn.Embedding(source_vocab_size, embedding_size)
        self.positional_encoding = PositionalEncoding(embedding_size, dropout, max_length=max_sequence_length)
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embedding_size,
                nhead=num_heads,
                dim_feedforward=dim_feedforward,
                dropout=dropout,
                activation=activation,
                layer_norm_eps=layer_norm_eps,
            ),
            num_encoder_layers,
        )
        self.classifier = nn.Linear(embedding_size, 2)

    def forward(self, source_sequence: torch.Tensor) -> torch.Tensor:
        """
        :param source_sequence: Source sequence shaped (sequence length, batch size)
        :returns: Tensor shaped (sequence length, batch size, embedding size)
        """
        source_embedding = self.source_embedding(source_sequence)
        positional_encoding = self.positional_encoding(source_embedding)
        encoder_output = self.encoder(positional_encoding)
        # Average the sequence length dimension.
        encoder_output = encoder_output.mean(dim=0)
        some_variable = self.classifier(encoder_output)
        return some_variable


class CNNSequenceClassifier(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int,
        num_filters: int,
        kernel_size: tuple[int],
        num_classes: int,
        hidden_size: int,
    ):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # P = ((S-1)*W-S+F)/2, with F = filter size, S = stride, W = input size
        self.conv = nn.Conv2d(1, num_filters, kernel_size, padding=((kernel_size[0] - 1) // 2, 0))
        self.linear1 = nn.Linear(num_filters, hidden_size)
        self.linear2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        :param x: Tensor shaped (batch size, sequence length)
        :returns: Tensor shaped (batch size, num classes)
        """
        x = self.embedding(x)
        # (sequence length, batch size, embedding size) -> (batch size, sequence length, embedding size)
        if x.shape[0] > 1:
            x = x.permute(1, 0, 2)
        # Add channel dimension. -> (batch size, 1, sequence length, embedding size)
        x = x.unsqueeze(1)
        x = self.conv(x)
        x = torch.relu(x)
        x = x.max(2).values
        x = self.linear1(x.squeeze(2))
        x = torch.relu(x)
        x = self.linear2(x)
        return x


if __name__ == "__main__":
    _test()

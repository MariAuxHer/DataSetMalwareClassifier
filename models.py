#!/opt/homebrew/bin/python3
import torch
import torch.nn as nn


class PositionalEncoding(nn.Module):
    def __init__(self, embed_dim: int, dropout: float, max_length: int = 2048):
        """
        :param embed_dim: Number of elements in embedding vectors.
        :param dropout: Probability of zeroing an output element.
        :param max_length: Longest sequence length supported.
        """
        super().__init__()
        ln_10000 = 9.21034049987793
        positional_encoding = torch.zeros((max_length, embed_dim))
        positions = torch.arange(0, max_length).reshape(max_length, 1)
        scale = torch.exp(-torch.arange(0, embed_dim, 2) * ln_10000 / embed_dim)
        positional_encoding[:, 0::2] = torch.sin(positions * scale)
        positional_encoding[:, 1::2] = torch.cos(positions * scale)
        positional_encoding = positional_encoding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer("positional_encoding", positional_encoding)

    def forward(self, token_embedding: torch.Tensor):
        """
        :param token_embedding: Token embeddings shaped (sequence length, batch size, embedding size)
        :returns: Embeddings with positional encoding added, possibly with some dropouts.
        """
        return self.dropout(token_embedding + self.positional_encoding[: token_embedding.size(0), :])


class SequenceToTensor(nn.Module):
    def __init__(
        self,
        source_vocab_size: int,
        num_encoder_layers: int = 6,
        num_heads: int = 8,
        embedding_size: int = 64,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        activation: callable = nn.functional.relu,
        layer_norm_eps: float = 1e-5,
    ):
        super().__init__()

        self.source_embedding = nn.Embedding(source_vocab_size, embedding_size)
        self.positional_encoding = PositionalEncoding(embedding_size, dropout)
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embedding_size,
                nhead=num_heads,
                dim_feedforward=dim_feedforward,
                dropout=dropout,
                activation=activation,
                layer_norm_eps=layer_norm_eps,
            ),
            num_encoder_layers,
        )
        self.classifier = nn.Linear(embedding_size, 2)

    def forward(self, source_sequence: torch.Tensor) -> torch.Tensor:
        """
        :param source_sequence: Source sequence shaped (sequence length, batch size)
        :returns: Tensor shaped (sequence length, batch size, embedding size)
        """
        source_embedding = self.source_embedding(source_sequence)
        positional_encoding = self.positional_encoding(source_embedding)
        encoder_output = self.encoder(positional_encoding)
        # Average the sequence length dimension.
        encoder_output = encoder_output.mean(dim=0)
        some_variable = self.classifier(encoder_output)
        return some_variable

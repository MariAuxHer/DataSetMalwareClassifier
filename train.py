#!/opt/homebrew/bin/python3
import torch
from torch.utils.data import DataLoader, TensorDataset
from pathlib import Path
from reforming import processOpseqLabels
from models import SequenceToTensor
import torch.nn as nn
from tqdm import tqdm


LEARNING_RATE = 0.0001
EPOCHS = 5
BATCH_SIZE = 10
source_vocab_size = 219 

def dataSetUp(opcode_sequences: torch.Tensor, labels: torch.Tensor): 
    dataset = TensorDataset(opcode_sequences, labels)
    train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    validation_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    return train_dataloader, validation_dataloader


def try_model(train_dataloader: DataLoader, validation_dataloader: DataLoader) -> tuple[list, list]:
    train_accuracy = []
    validation_accuracy = []
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SequenceToTensor(source_vocab_size).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    for epoch in (progress_bar := tqdm(range(EPOCHS))):
    #for epoch in (range(EPOCHS)):
        # training part of the model
        model.train() # (some layers can be deactivated)
        epoch_loss = 0
        n_correct = 0
        n_examples = 0

        for examples, labels in (progress_bar := tqdm(train_dataloader, desc=f"training epoch {epoch}")):
        #for examples, labels in (train_loader): # train loader gives us batches of 64 instances (right now)
            
            # resetting the gradient
            optimizer.zero_grad()
            examples = examples.to(device)
            labels = labels.to(device)
            # we are passing batches of 64 instances into the model. 
            logits = model(examples)
            loss = criterion(logits, labels)
            # calculating for how much approximately the weights should be updated 
            loss.backward()
            # optimizer finds the best solution to update the weights
            optimizer.step()

            # finding out how many logits were predicted correctly at this 64 batch 
            n_correct += (logits.max(1).indices == labels).sum().item()

            epoch_loss += loss.item()
            n_examples += labels.shape[0]

            progress_bar.set_postfix_str(
                f"mean loss: {epoch_loss / n_examples:.5f}, accuracy: {n_correct / n_examples:.4f}"
            )

        train_accuracy.append(n_correct / n_examples)

        # evaluating the model
        model.eval()
        validation_loss = 0 
        n_correct = 0
        n_examples = 0
    
        for examples, labels in (progress_bar := tqdm(validation_dataloader, desc="validating")):
        # for examples, labels in (validation_loader):
            examples = examples.to(device)
            labels = labels.to(device)

            # we are disabling the gradients bc we are not updating 
            with torch.autograd.no_grad():
                logits = model(examples)
                loss = criterion(logits, labels)
            # max(1) bc that specific instance belongs to the class with the highest probability.
            n_correct += (logits.max(1).indices == labels).sum().item()

            validation_loss += loss.item()
            n_examples += labels.shape[0]
            progress_bar.set_postfix_str(
                f"mean loss: {validation_loss / n_examples:.5f}, accuracy: {n_correct / n_examples:.4f}"
            )

        validation_accuracy.append(n_correct / n_examples)
        #print(epoch)
    return train_accuracy, validation_accuracy

if __name__ == "__main__":
    tensor_of_tensors_opcode_sequences = torch.load('tensor_of_tensors_opcode_sequences.pt')
    labels = torch.load('labels.pt')
    print(source_vocab_size)
    train_dataloader, validation_dataloader = dataSetUp(tensor_of_tensors_opcode_sequences, labels)
    train_accuracy, validation_accuracy = try_model(train_dataloader, validation_dataloader)


    
    #print("\n\nTENSORS\n\n")
    #print(tensor_of_tensors_opcode_sequences)
    #print("\n\n")
    #print(labels)
    #print("\n\n")
    #print(tensor_of_tensors_opcode_sequences.size())
    #print(labels.size())
    


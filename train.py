import argparse
import torch
from torch.utils.data import DataLoader, TensorDataset
from pathlib import Path

# from reforming import processOpseqLabels
from models import SequenceToTensor
import torch.nn as nn
from tqdm import tqdm


LEARNING_RATE = 0.0001
EPOCHS = 25
BATCH_SIZE = 8
source_vocab_size = 220
dtype = torch.bfloat16
parser = argparse.ArgumentParser()
parser.add_argument("--experiment-name", type=str, default="debug")
args = parser.parse_args()


def main():
    data_path = Path("data")

    labels = torch.load(data_path / "labels.pt")
    tensor_of_tensors_opcode_sequences = torch.load(data_path / "opcode_sequences.pt")
    # print(tensor_of_tensors_opcode_sequences.shape)
    print(source_vocab_size)
    print(tensor_of_tensors_opcode_sequences.size())
    print(labels.size())
    train_dataloader, validation_dataloader = dataSetUp(tensor_of_tensors_opcode_sequences, labels)
    try_model(train_dataloader, validation_dataloader)


def dataSetUp(opcode_sequences: torch.Tensor, labels: torch.Tensor):
    # Shuffling features and labels simultaneously.
    indices = torch.randperm(1986)
    features = opcode_sequences[indices, :]
    labels = labels[indices]

    n_train = int(features.shape[0] * 0.80)
    train_features = features[:n_train, :]
    train_labels = labels[:n_train]
    validation_features = features[n_train:, :]
    validation_labels = labels[n_train:]

    datasetTrain = TensorDataset(train_features, train_labels)
    datasetValidation = TensorDataset(validation_features, validation_labels)
    train_dataloader = DataLoader(datasetTrain, batch_size=BATCH_SIZE, shuffle=True)
    validation_dataloader = DataLoader(datasetValidation, batch_size=BATCH_SIZE)
    return train_dataloader, validation_dataloader


def try_model(train_dataloader: DataLoader, validation_dataloader: DataLoader) -> tuple[list, list]:
    results_folder = Path("results") / args.experiment_name
    if not results_folder.exists():
        results_folder.mkdir(exist_ok=True, parents=True)

    with open(results_folder / "results.csv", "w") as file:
        file.write("epoch,train_accuracy,validation_accuracy,train_loss,validation_loss\n")

    best_validation_accuracy = 0
    train_accuracies = []
    validation_accuracies = []

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SequenceToTensor(source_vocab_size).to(dtype).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    for epoch in (progress_bar := tqdm(range(EPOCHS))):
        # for epoch in (range(EPOCHS)):
        # training part of the model
        model.train()  # (some layers can be deactivated)
        train_loss = 0
        n_correct = 0
        n_examples = 0

        for examples, labels in (progress_bar := tqdm(train_dataloader, desc=f"training epoch {epoch}")):
            # for examples, labels in (train_loader): # train loader gives us batches of 64 instances (right now)

            # resetting the gradient
            optimizer.zero_grad()
            examples = examples.to(device)
            labels = labels.to(device)
            examples = examples.permute(1, 0)
            # we are passing batches of 64 instances into the model.
            logits = model(examples)
            loss = criterion(logits, labels)
            # calculating for how much approximately the weights should be updated
            loss.backward()
            # optimizer finds the best solution to update the weights
            optimizer.step()

            # finding out how many logits were predicted correctly at this 64 batch
            n_correct += (logits.max(1).indices == labels).sum().item()

            train_loss += loss.item()
            n_examples += labels.shape[0]

            progress_bar.set_postfix_str(
                f"mean loss: {train_loss / n_examples:.5f}, accuracy: {n_correct / n_examples:.4f}"
            )

        train_accuracies.append(n_correct / n_examples)

        # evaluating the model
        model.eval()
        validation_loss = 0
        n_correct = 0
        n_examples = 0

        for examples, labels in (progress_bar := tqdm(validation_dataloader, desc="validating")):
            # for examples, labels in (validation_loader):
            examples = examples.to(device)
            labels = labels.to(device)
            examples = examples.permute(1, 0)
            # we are disabling the gradients bc we are not updating
            with torch.autograd.no_grad():
                logits = model(examples)
                loss = criterion(logits, labels)
            # max(1) bc that specific instance belongs to the class with the highest probability.
            n_correct += (logits.max(1).indices == labels).sum().item()

            validation_loss += loss.item()
            n_examples += labels.shape[0]
            progress_bar.set_postfix_str(
                f"mean loss: {validation_loss / n_examples:.5f}, accuracy: {n_correct / n_examples:.4f}"
            )

        validation_accuracies.append(n_correct / n_examples)
        with open(results_folder / "results.csv", "a") as file:
            # epoch,train_accuracy,validation_accuracy,train_loss,validation_loss
            file.write(f"{epoch},{train_accuracies[-1]},{validation_accuracies[-1]},{train_loss},{validation_loss}\n")
        if validation_accuracies[-1] > best_validation_accuracy:
            best_validation_accuracy = validation_accuracies[-1]
            torch.save(model, results_folder / "best_model.pth")

    return train_accuracies, validation_accuracies


if __name__ == "__main__":
    main()

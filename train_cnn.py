import argparse
import torch
from pathlib import Path
from file_to_tensor import open_opseq
import random

from models import CNNSequenceClassifier
import torch.nn as nn
import tqdm


LEARNING_RATE = 1e-2
EPOCHS = 10
EMBEDDING_SIZE = 16
HIDDEN_SIZE = 16
N_FILTERS = 64
VOCAB_SIZE = 220
FAKE_BATCH_SIZE = 16
dtype = torch.float32
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
parser = argparse.ArgumentParser()
parser.add_argument("--experiment-name", type=str, default="debug_cnn")
args = parser.parse_args()


def main():
    opcode_sequences, labels = read_files()

    train_opcode_sequences, train_labels, validation_opcode_sequences, validation_labels = data_setup(
        opcode_sequences, labels
    )
    try_model(train_opcode_sequences, train_labels, validation_opcode_sequences, validation_labels)


def read_files() -> tuple[list, list]:
    """
    :return: tuple of lists of opcode sequences and labels
    """
    opcode_sequences = []
    labels = []

    benign_files = tuple(Path("./Malign").glob("*.opseq"))
    for f in tqdm.tqdm(benign_files, desc="reading benign files"):
        indices_list = open_opseq(f)
        opcode_sequences.append(indices_list)
        labels.append(1)

    malware_files = tuple(Path("./Benign").glob("*.opseq"))
    for f in tqdm.tqdm(malware_files, desc="reading malware files"):
        indices_list = open_opseq(f)
        opcode_sequences.append(indices_list)
        labels.append(0)

    return opcode_sequences, labels


def data_setup(opcode_sequences: list, labels: list) -> tuple[list, list, list, list]:
    # Shuffle opcode_sequences and labels the same way.
    pairs = list(zip(opcode_sequences, labels))
    random.shuffle(pairs)
    opcode_sequences, labels = zip(*pairs)

    # Split into train and validation sets.
    split = int(len(opcode_sequences) * 0.8)
    train_opcode_sequences = opcode_sequences[:split]
    train_labels = labels[:split]
    validation_opcode_sequences = opcode_sequences[split:]
    validation_labels = labels[split:]

    # Convert to tensors.
    train_opcode_sequences = [torch.tensor(sequence, dtype=torch.int32) for sequence in train_opcode_sequences]
    validation_opcode_sequences = [torch.tensor(sequence, dtype=torch.int32) for sequence in validation_opcode_sequences]

    return train_opcode_sequences, train_labels, validation_opcode_sequences, validation_labels


def try_model(
    train_opcode_sequences: list, train_labels: list, validation_opcode_sequences: list, validation_labels: list
) -> tuple[list, list]:
    results_folder = Path("results") / args.experiment_name
    if not results_folder.exists():
        results_folder.mkdir(exist_ok=True, parents=True)

    with open(results_folder / "results.csv", "w") as file:
        file.write("epoch,train_accuracy,validation_accuracy,train_loss,validation_loss\n")

    best_validation_accuracy = 0
    train_accuracies = []
    validation_accuracies = []

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = (
        CNNSequenceClassifier(VOCAB_SIZE, EMBEDDING_SIZE, N_FILTERS, (8, EMBEDDING_SIZE), 2, HIDDEN_SIZE)
        .to(dtype)
        .to(device)
    )
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.RMSprop(model.parameters(), lr=LEARNING_RATE)

    for epoch in (progress_bar := tqdm.tqdm(range(EPOCHS))):
        # for epoch in (range(EPOCHS)):
        # training part of the model
        model.train()  # (some layers can be deactivated)
        train_loss = 0
        n_correct = 0
        n_examples = 0

        for example_number, (example, label) in (
            progress_bar := tqdm.tqdm(enumerate(tuple(zip(train_opcode_sequences, train_labels))), desc=f"training epoch {epoch}")
        ):
            example = example[None, :].to(device)
            label = torch.tensor(label).unsqueeze(0).to(device)
            logits = model(example)
            loss = criterion(logits, label)
            loss.backward()
            
            n_correct += (logits.max(1).indices == label).sum().item()
            train_loss += loss.item()
            n_examples += 1

            if example_number % FAKE_BATCH_SIZE == (FAKE_BATCH_SIZE - 1):
                optimizer.step()
                optimizer.zero_grad()
            progress_bar.set_postfix_str(
                f"mean loss: {train_loss / n_examples:.5f}, accuracy: {n_correct / n_examples:.4f}"
            )

        train_accuracies.append(n_correct / n_examples)

        # evaluating the model
        model.eval()
        validation_loss = 0
        n_correct = 0
        n_examples = 0

        for example_number, (example, label) in (
            progress_bar := tqdm.tqdm(enumerate(tuple(zip(validation_opcode_sequences, validation_labels))), desc="validating")
        ):
            example = example[None, :].to(device)
            label = torch.tensor(label).unsqueeze(0).to(device)
            with torch.autograd.no_grad():
                logits = model(example)
                loss = criterion(logits, label)
            n_correct += (logits.max(1).indices == label).sum().item()
            validation_loss += loss.item()
            n_examples += 1

            progress_bar.set_postfix_str(
                f"mean loss: {validation_loss / n_examples:.5f}, accuracy: {n_correct / n_examples:.4f}"
            )

        validation_accuracies.append(n_correct / n_examples)
        with open(results_folder / "results.csv", "a") as file:
            # epoch,train_accuracy,validation_accuracy,train_loss,validation_loss
            file.write(f"{epoch},{train_accuracies[-1]},{validation_accuracies[-1]},{train_loss},{validation_loss}\n")
        if validation_accuracies[-1] > best_validation_accuracy:
            best_validation_accuracy = validation_accuracies[-1]
            torch.save(model, results_folder / "best_model.pth")

    return train_accuracies, validation_accuracies


if __name__ == "__main__":
    main()
